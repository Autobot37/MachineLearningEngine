{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning torch lightning fabric\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeEC2x-cPRjZ",
        "outputId": "c22545b1-abf7-40b4-8761-d1b3f5e9c6a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: lightning in /usr/local/lib/python3.10/dist-packages (2.0.2)\n",
            "Requirement already satisfied: fabric in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.65.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.4.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (0.11.4)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.5.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (0.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: arrow<3.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.2.3)\n",
            "Requirement already satisfied: beautifulsoup4<6.0,>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.11.2)\n",
            "Requirement already satisfied: click<10.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (8.1.3)\n",
            "Requirement already satisfied: croniter<1.4.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.3.14)\n",
            "Requirement already satisfied: dateutils<2.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (0.6.12)\n",
            "Requirement already satisfied: deepdiff<8.0,>=5.7.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.3.0)\n",
            "Requirement already satisfied: fastapi<0.89.0,>=0.69.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (0.88.0)\n",
            "Requirement already satisfied: inquirer<5.0,>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (3.1.3)\n",
            "Requirement already satisfied: lightning-cloud>=0.5.34 in /usr/local/lib/python3.10/dist-packages (from lightning) (0.5.36)\n",
            "Requirement already satisfied: psutil<7.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (5.9.5)\n",
            "Requirement already satisfied: pydantic<4.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.10.7)\n",
            "Requirement already satisfied: requests<4.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.27.1)\n",
            "Requirement already satisfied: rich<15.0,>=12.3.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (13.3.4)\n",
            "Requirement already satisfied: starlette in /usr/local/lib/python3.10/dist-packages (from lightning) (0.22.0)\n",
            "Requirement already satisfied: starsessions<2.0,>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.3.0)\n",
            "Requirement already satisfied: traitlets<7.0,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (5.7.1)\n",
            "Requirement already satisfied: urllib3<3.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.26.15)\n",
            "Requirement already satisfied: uvicorn<2.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (0.22.0)\n",
            "Requirement already satisfied: websocket-client<3.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.5.1)\n",
            "Requirement already satisfied: websockets<12.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (11.0.3)\n",
            "Requirement already satisfied: invoke>=2.0 in /usr/local/lib/python3.10/dist-packages (from fabric) (2.1.2)\n",
            "Requirement already satisfied: paramiko>=2.4 in /usr/local/lib/python3.10/dist-packages (from fabric) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from arrow<3.0,>=1.2.0->lightning) (2.8.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<6.0,>=4.8.0->lightning) (2.4.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from dateutils<2.0->lightning) (2022.7.1)\n",
            "Requirement already satisfied: ordered-set<4.2.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from deepdiff<8.0,>=5.7.0->lightning) (4.1.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette->lightning) (3.6.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.4)\n",
            "Requirement already satisfied: blessed>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from inquirer<5.0,>=2.10.0->lightning) (1.20.0)\n",
            "Requirement already satisfied: python-editor>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from inquirer<5.0,>=2.10.0->lightning) (1.0.4)\n",
            "Requirement already satisfied: readchar>=3.0.6 in /usr/local/lib/python3.10/dist-packages (from inquirer<5.0,>=2.10.0->lightning) (4.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: pyjwt in /usr/local/lib/python3.10/dist-packages (from lightning-cloud>=0.5.34->lightning) (2.7.0)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from lightning-cloud>=0.5.34->lightning) (0.0.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from lightning-cloud>=0.5.34->lightning) (1.16.0)\n",
            "Requirement already satisfied: bcrypt>=3.2 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric) (4.0.1)\n",
            "Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric) (40.0.2)\n",
            "Requirement already satisfied: pynacl>=1.5 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric) (1.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<4.0->lightning) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<4.0->lightning) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<4.0->lightning) (3.4)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<15.0,>=12.3.0->lightning) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<15.0,>=12.3.0->lightning) (2.14.0)\n",
            "Requirement already satisfied: itsdangerous<3.0.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from starsessions<2.0,>=1.2.1->lightning) (2.1.2)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn<2.0->lightning) (0.14.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette->lightning) (1.3.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.19.0->inquirer<5.0,>=2.10.0->lightning) (0.2.6)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric) (1.15.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich<15.0,>=12.3.0->lightning) (0.1.2)\n",
            "Requirement already satisfied: setuptools>=41.0 in /usr/local/lib/python3.10/dist-packages (from readchar>=3.0.6->inquirer<5.0,>=2.10.0->lightning) (67.7.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric) (2.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lightning.pytorch as pl\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import lightning as L\n",
        "from lightning import Fabric as f\n",
        "import torch.nn as nn\n",
        "from dataclasses import dataclass\n",
        "from torch.nn import functional as F\n",
        "import math"
      ],
      "metadata": {
        "id": "CW0nsmEIvGtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
      ],
      "metadata": {
        "id": "6JKFl30sAavj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n",
        "import sentencepiece\n",
        "from sentencepiece import SentencePieceProcessor,SentencePieceTrainer"
      ],
      "metadata": {
        "id": "GB5XZvepPjhT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd064813-25c4-4de7-a16b-807236c44a66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional"
      ],
      "metadata": {
        "id": "ubPOVrnOA9M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "#Tokenizer[model path]\n",
        "\n",
        "class Tokenizer:\n",
        "\n",
        "  def __init__(self, model_path):\n",
        "    self.processor = SentencePieceProcessor(model_file=str(model_path))\n",
        "    self.bos_id = self.processor.bos_id()\n",
        "    self.eos_id = self.processor.eos_id()\n",
        "    self.pad_id = self.processor.pad_id()\n",
        "  \n",
        "  @property\n",
        "  def vocab_size(self) -> int:\n",
        "    return self.processor.vocab_size()\n",
        "\n",
        "\n",
        "#encode\n",
        "  def encode(\n",
        "        self,\n",
        "        string: str,\n",
        "        bos: bool = False,\n",
        "        eos: bool = False,\n",
        "        max_length: int = -1,\n",
        "        pad: bool = False,\n",
        "        device: Optional[torch.device] = None\n",
        "    ) -> torch.Tensor:\n",
        "    tokens = self.processor.encode(string)\n",
        "    if bos:\n",
        "        tokens = [self.bos_id] + tokens\n",
        "    if eos:\n",
        "        tokens = tokens + [self.eos_id]\n",
        "    if max_length > 0:\n",
        "        tokens = tokens[:max_length]\n",
        "    if pad and len(tokens) < max_length:\n",
        "        tokens += [self.pad_id] * (max_length - len(tokens))\n",
        "\n",
        "    return torch.tensor(tokens, dtype=torch.long, device=device)\n",
        "\n",
        "\n",
        "#decode\n",
        "  def decode(self,tokens:torch.Tensor) -> str:\n",
        "    return self.processor.decode(tokens.tolist())\n",
        "  \n",
        "#train[take input txt bro][have export it on path]\n",
        "  @staticmethod\n",
        "  def train(input: str, destination: str, vocab_size=2000) -> None:\n",
        "    model_prefix = os.path.join(destination, \"tokenizer\")\n",
        "    SentencePieceTrainer.Train(input=input, model_prefix=model_prefix, vocab_size=vocab_size)\n",
        "\n",
        "#@title\n",
        "Tokenizer.train(input=\"/content/data/input.txt\",destination=\"/content/tokenizer\")\n",
        "\n",
        "#@title\n",
        "enc = Tokenizer(\"/content/tokenizer/tokenizer.model\")\n",
        "\n"
      ],
      "metadata": {
        "id": "-4wZ29CtAv23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xbQ2GZlABCTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scale = 12\n",
        "block_size = 8\n",
        "learning_rate = 6e-4 # max learning rate\n",
        "max_iters = 600 # total number of training iterations\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95 \n",
        "n_layer = 2\n",
        "n_head = 2\n",
        "n_embd = 96\n",
        "betas = (beta1,beta2)\n",
        "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
        "bias = False\n",
        "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
        "block_size = 24\n",
        "vocab_size = 2000\n",
        "out_dir = 'out'\n",
        "eval_interval = 200\n",
        "log_interval = 1\n",
        "eval_iters = 200\n",
        "eval_only = False # if True, script exits right after the first eval\n",
        "always_save_checkpoint = True # if True, always save a checkpoint after each eval\n",
        "init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n",
        "# wandb logging\n",
        "wandb_log = True # disabled by default\n",
        "wandb_project = 'owt'\n",
        "wandb_run_name = 'gpt2' # 'run' + str(time.time())\n",
        "# data\n",
        "dataset = 'openwebtext'\n",
        "gradient_accumulation_steps = 5 * 8 # used to simulate larger batch sizes\n",
        "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
        "block_size = 64\n",
        "# model\n",
        " # do we use bias inside LayerNorm and Linear layers?\n",
        "# adamw optimizer\n",
        "learning_rate = 6e-4 # max learning rate\n",
        "max_iters = 600 # total number of training iterations\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
        "# learning rate decay settings\n",
        "decay_lr = True # whether to decay the learning rate\n",
        "warmup_iters = 20 # how many steps to warm up for\n",
        "lr_decay_iters = 600 # should be ~= max_iters per Chinchilla\n",
        "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
        "# DDP settings\n",
        "backend = 'gloo' # 'nccl', 'gloo', etc.\n",
        "# system\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "dtype = 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "compile = False"
      ],
      "metadata": {
        "id": "bxKvWNneOfWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {k:v for k,v in globals().items() if not k.startswith(\"_\") and isinstance(v,(int,float,bool,str))}\n"
      ],
      "metadata": {
        "id": "MluGtIWPOiKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config(dict):\n",
        "  def __init__(self,config):\n",
        "    self.__dict__.update(config)\n",
        "    "
      ],
      "metadata": {
        "id": "TT_dkQMFOGkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config(config)"
      ],
      "metadata": {
        "id": "KoAZq7M0Oys_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config.block_size"
      ],
      "metadata": {
        "id": "IppZEadeO1iP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6cfc9f3-f9c1-4eff-a8e2-481634be2776"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NextToken(torch.utils.data.Dataset):##custom daatset\n",
        "  def __init__(self,bin_file,config):\n",
        "    super().__init__()\n",
        "    self.data = np.memmap(bin_file,dtype=np.uint16,mode='r')#memmap\n",
        "    self.config = config\n",
        "  \n",
        "  def __len__(self):\n",
        "    return (len(self.data) - self.config.block_size)\n",
        "\n",
        "  def __getitem__(self,i):\n",
        "    data = self.data\n",
        "    start_index = i\n",
        "    end_index = start_index + self.config.block_size\n",
        "    x = torch.from_numpy((self.data[start_index:end_index]).astype(np.int64))\n",
        "    y = torch.from_numpy((self.data[start_index + 1:end_index + 1]).astype(np.int64))\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "jJWoyyVVMbjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.distributed import DistributedSampler\n",
        "class Sampler(DistributedSampler):\n",
        "  def __init__(self,dataset,world_size,rank,shuffle):\n",
        "    super().__init__(dataset=dataset,num_replicas=world_size,rank=rank,shuffle=shuffle)\n",
        "\n"
      ],
      "metadata": {
        "id": "a8nxqHWihUfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3nydHcqmj0e"
      },
      "outputs": [],
      "source": [
        "class DataModule(pl.LightningDataModule):\n",
        "  def __init__(self,dir_path,config,trainer=None):\n",
        "    self.dir_path = dir_path\n",
        "    self.prepare_data_per_node = True\n",
        "    self.config = config\n",
        "    self.trainer = trainer\n",
        "    self.current_epoch = 0\n",
        "  \n",
        "  def prepare_data(self):\n",
        "    input_file_path = os.path.join(self.dir_path,\"input.txt\")\n",
        "\n",
        "    with open(input_file_path,'r') as f:\n",
        "      text = f.read()\n",
        "    n = len(text)\n",
        "\n",
        "    train_data = text[:int(n*0.9)]\n",
        "    val_data = text[int(n*0.9):]\n",
        "    #encode\n",
        "    train_ids = enc.encode(train_data)\n",
        "    val_ids = enc.encode(val_data)\n",
        "    #print\n",
        "    if self.trainer.is_global_zero:\n",
        "      print(f\"train_ids has {len(train_ids)} tokens\")\n",
        "      print(f\"val_ids has {len(val_ids)} tokens\")\n",
        "    #convert to numpy\n",
        "    train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "    val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "    #export_to_file\n",
        "    train_ids.tofile(os.path.join(self.dir_path,'train.bin'))\n",
        "    val_ids.tofile(os.path.join(self.dir_path,'val.bin'))\n",
        "\n",
        "  def setup(self, stage):\n",
        "    if stage == \"fit\":\n",
        "      self.train_data = NextToken(bin_file=os.path.join(self.dir_path, \"train.bin\"), config=self.config)#train_dataset = torch.utild.dataset(nextTokendataset)\n",
        "      if self.trainer.is_global_zero:\n",
        "        self.val_data = NextToken(bin_file=os.path.join(self.dir_path, \"val.bin\"), config=self.config)\n",
        "    \n",
        "    if stage == \"predict\":\n",
        "      self.predict_data = NextToken(bin_file=os.path.join(self.dir_path, \"predict.bin\"), config=self.config)\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    if isinstance(self.trainer,pl.Trainer):\n",
        "      return DataLoader(self.train_data, batch_size=64, pin_memory=True, num_workers=2,shuffle=True)\n",
        "    else:\n",
        "      self.sampler = Sampler(dataset=self.train_data,world_size=self.trainer.world_size, rank=self.trainer.local_rank, shuffle=True)\n",
        "      self.sampler.set_epoch(self.current_epoch)\n",
        "      return DataLoader(self.train_data, batch_size=64, pin_memory=True, num_workers=2, sampler=self.sampler)  \n",
        "  \n",
        "  def val_dataloader(self):\n",
        "    if self.trainer.is_global_zero:\n",
        "      return DataLoader(self.val_data, batch_size=64, pin_memory=False, num_workers=0,shuffle=True)\n",
        "    else:\n",
        "      print(\"only master node have val_data\")\n",
        "      return None\n",
        "  \n",
        "  def on_epoch_start(self):\n",
        "    self.current_eoch = self.trainer.current_epoch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = pl.Trainer(accelerator=\"cuda\",max_epochs=10,use_distributed_sampler=True)"
      ],
      "metadata": {
        "id": "L0mQYJQ5OZ08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e240999-c0ee-40bb-982e-8ba33321b35c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.current_epoch"
      ],
      "metadata": {
        "id": "y3oA2ossXHfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79e32157-40fd-4ca1-d9fe-c2db5aec5db1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dm  = DataModule(\"/content/data\",config=config,trainer=trainer)"
      ],
      "metadata": {
        "id": "yga-nkNmvBDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dm.prepare_data()"
      ],
      "metadata": {
        "id": "2KGiM0emQVh7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44f01e17-4a9e-4c27-baef-d5db198b6fb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_ids has 18949 tokens\n",
            "val_ids has 2137 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dm.setup(stage=\"fit\")"
      ],
      "metadata": {
        "id": "DikPLffnJNsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dm.current_epoch = 12"
      ],
      "metadata": {
        "id": "VGzFgGfseQ8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt = dm.train_dataloader()"
      ],
      "metadata": {
        "id": "48DkHRzhKMiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3bxxvAX9bBEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for a,b in dt:\n",
        "  print(a,b)\n",
        "  break"
      ],
      "metadata": {
        "id": "74TPJPi_YYIm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d4796f0-a90a-4f60-a8eb-3687b8f2aa75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[   5, 1073,   10,  ...,    3,  106,    5],\n",
            "        [ 654,    3,  662,  ...,   67,    4,    7],\n",
            "        [  31,   12,  323,  ...,    6, 1669,  832],\n",
            "        ...,\n",
            "        [   7,  620,   84,  ...,   45, 1378, 1110],\n",
            "        [  10,    3,  185,  ...,  103,   11,   15],\n",
            "        [1806,   85,   11,  ...,   11,  557,    3]]) tensor([[1073,   10,  643,  ...,  106,    5,    9],\n",
            "        [   3,  662,   75,  ...,    4,    7,  164],\n",
            "        [  12,  323,    3,  ..., 1669,  832,   38],\n",
            "        ...,\n",
            "        [ 620,   84, 1414,  ..., 1378, 1110,   24],\n",
            "        [   3,  185,  572,  ...,   11,   15,  541],\n",
            "        [  85,   11,  393,  ...,  557,    3,  367]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dm.current_epoch = 9\n",
        "for a,b in dt:\n",
        "  print(a,b)\n",
        "  break"
      ],
      "metadata": {
        "id": "th7fa8RaYVeL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e169d1d-426e-4866-91f3-dbfb41fc8687"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[   6,    8,  122,  ...,  143, 1083,  979],\n",
            "        [   4,   14,   13,  ...,  229,    3,   32],\n",
            "        [ 210,   22,   20,  ...,  120,   10,  281],\n",
            "        ...,\n",
            "        [  10,   51,  104,  ...,   81,   42,  778],\n",
            "        [   3,   92,  742,  ...,  171,  737,    3],\n",
            "        [  15,  204,   97,  ...,  920,    3,    6]]) tensor([[   8,  122,    4,  ..., 1083,  979,  590],\n",
            "        [  14,   13,  700,  ...,    3,   32,    4],\n",
            "        [  22,   20,   13,  ...,   10,  281,   10],\n",
            "        ...,\n",
            "        [  51,  104,   11,  ...,   42,  778, 1301],\n",
            "        [  92,  742,    3,  ...,  737,    3,    6],\n",
            "        [ 204,   97,  179,  ...,    3,    6,    8]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#########MODEL#####################"
      ],
      "metadata": {
        "id": "KcLBu465yDh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    assert config.n_embd & config.n_head == 0\n",
        "    self.c_attn = nn.Linear(config.n_embd,3*config.n_embd,bias=config.bias)\n",
        "    self.c_proj = nn.Linear(config.n_embd,config.n_embd,bias=config.bias)\n",
        "    self.attn_dropout = nn.Dropout(config.dropout)\n",
        "    self.resid_dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    self.n_head = config.n_head\n",
        "    self.n_embd = config.n_embd\n",
        "    self.dropout = config.dropout\n",
        "\n",
        "  def forward(self,x):#B,T,C\n",
        "    B,T,C = x.size()\n",
        "    q,k,v = self.c_attn(x).split(self.n_embd,dim=2)\n",
        "    q = q.view(B,T,self.n_head,C//self.n_head).transpose(1,2)\n",
        "    k = k.view(B,T,self.n_head,C//self.n_head).transpose(1,2)\n",
        "    v = v.view(B,T,self.n_head,C//self.n_head).transpose(1,2)#B T nh headDim->b nh t hs\n",
        "\n",
        "    y = torch.nn.functional.scaled_dot_product_attention(q,k,v,attn_mask=None,dropout_p=self.dropout if self.training else 0,is_causal=True) #B NH T HS\n",
        "    y = y.transpose(1,2).contiguous().view(B,T,C)\n",
        "\n",
        "    out = self.c_proj(y)\n",
        "    out = self.resid_dropout(y)\n",
        "    return out #b  t c\n",
        "\n",
        "#@title\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    self.c_fc   = nn.Linear(config.n_embd, 4 * config.n_embd,bias=config.bias)\n",
        "    self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd,bias=config.bias)\n",
        "    self.dropout= nn.Dropout(config.dropout)\n",
        "    self.act = nn.GELU()\n",
        "  def forward(self,x):\n",
        "    x = self.dropout(self.c_proj(self.act(self.c_fc(x))))\n",
        "    return x\n",
        "\n",
        "#@title\n",
        "class Block(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    self.attn = CausalSelfAttention(config)\n",
        "    self.ln_1 = LayerNorm(config.n_embd,config.bias)\n",
        "    self.ln_2 = LayerNorm(config.n_embd,config.bias)\n",
        "    self.mlp = MLP(config) # layernorm 2 a attention a mlp\n",
        "  \n",
        "  def forward(self,x):\n",
        "    x = x + self.attn(self.ln_1(x))\n",
        "    x = x + self.mlp(self.ln_2(x))\n",
        "    return x\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self,ndim,bias):\n",
        "    super().__init__()\n",
        "    self.weight = nn.Parameter(torch.ones(ndim))\n",
        "    self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "  \n",
        "  def forward(self,x):\n",
        "    return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "  "
      ],
      "metadata": {
        "id": "cx3G6Zzs0xHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class plModel(pl.LightningModule):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    self.losslist = []\n",
        "    self.config = config\n",
        "    print(f\"this model is on {self.device}\")\n",
        "    self.transformer = nn.ModuleDict(dict(\n",
        "        wte = nn.Embedding(config.vocab_size,config.n_embd),\n",
        "        wpe = nn.Embedding(config.block_size,config.n_embd),\n",
        "        h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "        drop = nn.Dropout(config.dropout),\n",
        "        ln_f = LayerNorm(config.n_embd,config.bias)\n",
        "    ))\n",
        "    self.lm_head = nn.Linear(config.n_embd,config.vocab_size,bias=False)\n",
        "\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "    for a,b in self.named_parameters():\n",
        "      if a.endswith(\"c_proj.weight\"):\n",
        "        torch.nn.init.normal_(b,mean=0.0,std=0.02/math.sqrt(2 * config.n_layer))\n",
        "  \n",
        "    print(sum(p.numel() for p in self.parameters()))\n",
        "\n",
        "  def _init_weights(self,module):\n",
        "    if isinstance(module,nn.Linear):\n",
        "      torch.nn.init.normal_(module.weight,mean=0.0,std=0.02)\n",
        "      if module.bias is not None:\n",
        "        torch.nn.init.zeros_(module.bias)\n",
        "    if isinstance(module,nn.Embedding):\n",
        "      torch.nn.init.normal_(module.weight,mean=0.0,std=0.02)\n",
        "    \n",
        "    \n",
        "  def forward(self,x,y=None):#X=[B,CONTEXT_SIZE] = B,T\n",
        "    b,t = x.size()\n",
        "    pos = torch.arange(0,t,dtype=torch.int32, device=self.device).unsqueeze(0)\n",
        "    t_emb = self.transformer.wte(x)\n",
        "    #B,CONTEXT_SIZE,VOCAB_SIZE = B,T,C\n",
        "    p_emb = self.transformer.wpe(pos)\n",
        "    x = self.transformer.drop(t_emb+p_emb)\n",
        "    for block in self.transformer.h:\n",
        "      x = block(x) # b t c\n",
        "    out = self.transformer.ln_f(x)###BTC WE HAVE TO PASS THRROUGH LM_HEAD FOR B T VOCAB_SIZE\n",
        "    logits = self.lm_head(out)\n",
        "    return logits\n",
        "  \n",
        "  def training_step(self,batch,batch_idx):\n",
        "    x,y = batch\n",
        "    x = x.to(self.device)\n",
        "    y = y.to(self.device)\n",
        "    logits = self(x)#b t vocab_dim\n",
        "    B,T,vdim = logits.shape\n",
        "\n",
        "    logits = logits.view(B*T,vdim)\n",
        "    target = y.view(B*T)\n",
        "    loss = F.cross_entropy(logits,target)\n",
        "    self.losslist.append(loss)\n",
        "    return loss\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "    param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "    decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "    nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "    optim_groups = [\n",
        "        {'params': decay_params, 'weight_decay': self.config.weight_decay},\n",
        "        {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "    ]\n",
        "    num_decay_params = sum(p.numel() for p in decay_params)\n",
        "    num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "    print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "    print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "    optimizer = torch.optim.AdamW(optim_groups, lr=self.config.learning_rate, betas=(self.config.beta1,self.config.beta2))\n",
        "\n",
        "    return optimizer\n",
        " \n",
        "\n",
        "  @torch.no_grad()\n",
        "  def generate(self,idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "      logits, _ = self(idx_cond)\n",
        "      logits = logits[:, -1, :] / temperature\n",
        "      if top_k is not None:\n",
        "          v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "          logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "    \n",
        "  "
      ],
      "metadata": {
        "id": "-X3XSduiyFq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "scale = 6\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "  block_size:int=1024\n",
        "  vocab_size:int=50304\n",
        "  n_layer:int=12//scale\n",
        "  n_embd:int=768//scale\n",
        "  n_head:int=12//scale\n",
        "  dropout:float=0.0\n",
        "  bias:bool=True"
      ],
      "metadata": {
        "id": "1VuixlQM6_gV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = plModel(config)\n",
        "model.device"
      ],
      "metadata": {
        "id": "RWW7-r1n7m-T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd0be3fd-538e-49df-8f06-e6aa80848366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this model is on cpu\n",
            "611808\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#trainer.fit(model,dt)"
      ],
      "metadata": {
        "id": "bK-6SMaO7zkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############Trainer######################"
      ],
      "metadata": {
        "id": "AjHMLm6mmRhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now approach is forget about data and model, focus on trainer only using fabric customed thats it"
      ],
      "metadata": {
        "id": "qyFC2nRImkXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FKbLkw_Hb2C",
        "outputId": "f52d58ed-1958-41a0-fa2c-4cbc262d4c1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.15.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.31)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.23.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "qIKsAa2Xxavt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08474351-0eae-402c-9d45-20f179caf7af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshivasinghbagri\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lightning as L"
      ],
      "metadata": {
        "id": "nsCQo14Mx-vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lightning.fabric import Fabric"
      ],
      "metadata": {
        "id": "sAOz57uGxQRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "mL9bzDwmy6tA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class Trainer:\n",
        "  def __init__(self,config,model,datamodule):\n",
        "    \n",
        "    self.config = config\n",
        "    self.fabric = Fabric(precision=\"16-mixed\",accelerator=\"cuda\",strategy=\"auto\")\n",
        "    self.model = model\n",
        "    self.optimizer = model.configure_optimizers()\n",
        "    self.device = self.fabric.device\n",
        "    self.dm = datamodule(dir_path=\"/content/data\",config=self.config,trainer=self.fabric)\n",
        "    self.dm.prepare_data()\n",
        "    self.dm.setup(stage=\"fit\")\n",
        "    self.dm.setup(stage=\"val\")\n",
        "\n",
        "  def train(self):\n",
        "    \n",
        "    master_process = self.fabric.global_rank == 0\n",
        "\n",
        "    if master_process:\n",
        "      os.makedirs(self.config.out_dir, exist_ok=True)\n",
        "    \n",
        "    self.fabric.seed_everything(1227+self.fabric.global_rank)\n",
        "\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "    iter_num = 0\n",
        "    best_val_loss = 1e9\n",
        "    t0 = time.time()\n",
        "\n",
        "    if self.config.wandb_log and master_process:\n",
        "      wandb.init(project=self.config.wandb_project, name=self.config.wandb_run_name)\n",
        "\n",
        "    self.model, self.optimizer = self.fabric.setup(self.model, self.optimizer)\n",
        "\n",
        "    for iter_num in range(self.config.max_iters):\n",
        "      \n",
        "      lr = self.get_lr(iter_num,self.config) if self.config.decay_lr else self.config.learning_rate\n",
        "      for param_group in self.optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "  \n",
        "      \n",
        "      if iter_num % self.config.eval_interval == 0 and master_process:\n",
        "        losses = self.estimate_loss(self.config)\n",
        "        print(f\"step {iter_num}: eval_loss {losses['val']:.4f}\")\n",
        "\n",
        "      \n",
        "        if losses[\"val\"] < best_val_loss or self.always_save_checkpoints:\n",
        "          best_val_loss = losses[\"val\"]\n",
        "          self.checkpoint(model = self.model.state_dict(),iter=iter_num)\n",
        "\n",
        "     \n",
        "      \n",
        "\n",
        "      self.optimizer.zero_grad(set_to_none=True)\n",
        "      \n",
        "      self.model.current_epoch = iter_num\n",
        "      for batch_idx, batch in enumerate(self.dm.train_dataloader()):\n",
        "        is_accumulating = batch_idx < self.config.gradient_accumulation_steps \n",
        "        with self.fabric.no_backward_sync(self.model,enabled=(is_accumulating)):\n",
        "          loss = self.model.training_step(batch, batch_idx)\n",
        "          self.wandblog(iter=iter_num,loss=loss)\n",
        "          self.fabric.backward(loss)\n",
        "        \n",
        "        if not is_accumulating:\n",
        "          self.optimizer.step()\n",
        "          self.optimizer.zero_grad()\n",
        "      \n",
        "\n",
        "      t1 = time.time()\n",
        "      dt = t1 - t0\n",
        "      t0 = t1\n",
        "\n",
        "      if master_process:\n",
        "        lossf = loss.item()\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms\")\n",
        "      \n",
        "  @torch.no_grad()\n",
        "  def estimate_loss(self,config):\n",
        "    out = {}\n",
        "    self.model.eval()\n",
        "    for split in [\"val\"]:\n",
        "      losses = torch.zeros(config.eval_iters)\n",
        "      for k,batch in enumerate(self.dm.val_dataloader()):\n",
        "        loss = model.training_step(batch, k)\n",
        "        losses[k] = loss.item()\n",
        "        if k>config.eval_iters:\n",
        "          break\n",
        "      out[split] = losses.mean()\n",
        "    self.model.train()\n",
        "    return out\n",
        "  \n",
        "  def get_lr(self,iter,config):\n",
        "    if iter < config.warmup_iters:\n",
        "        return config.learning_rate * iter / config.warmup_iters\n",
        "    if iter > config.lr_decay_iters:\n",
        "        return config.min_lr\n",
        "    decay_ratio = (iter - config.warmup_iters) / (config.lr_decay_iters - config.warmup_iters)\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return config.min_lr + coeff * (config.learning_rate - config.min_lr)\n",
        "  \n",
        "  def checkpoint(self,model,iter):\n",
        "    checkpoint = {\n",
        "        \"model\":model,\n",
        "        \"iter_num\":iter,\n",
        "    }\n",
        "    print(f\"saving checkpoint{iter}\")\n",
        "    torch.save(checkpoint, os.path.join(out_dir, \"ckpt.pt\"))\n",
        "  \n",
        "  def wandblog(self,iter,loss):\n",
        "    wandb.log({\n",
        "        \"iter\": iter,\n",
        "        \"train_loss\": loss,\n",
        "    })\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "39FzmCUMmr4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = Trainer(config, model, DataModule)"
      ],
      "metadata": {
        "id": "xy-L3Hem3ipa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6e3eddb-4ab6-422a-bb13-e02fa733cbd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Using 16-bit Automatic Mixed Precision (AMP)\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Using 16-bit Automatic Mixed Precision (AMP)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 11, with 611,328 parameters\n",
            "num non-decayed parameter tensors: 5, with 480 parameters\n",
            "train_ids has 18949 tokens\n",
            "val_ids has 2137 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t.train()"
      ],
      "metadata": {
        "id": "_e3-Nr-C3k-o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ad1a27ed-b52f-4363-cde4-c91919c7aaa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Global seed set to 1227\n",
            "INFO:lightning.fabric.utilities.seed:Global seed set to 1227\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230522_155610-lgdbvz7t</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/shivasinghbagri/owt/runs/lgdbvz7t' target=\"_blank\">gpt2</a></strong> to <a href='https://wandb.ai/shivasinghbagri/owt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/shivasinghbagri/owt' target=\"_blank\">https://wandb.ai/shivasinghbagri/owt</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/shivasinghbagri/owt/runs/lgdbvz7t' target=\"_blank\">https://wandb.ai/shivasinghbagri/owt/runs/lgdbvz7t</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: eval_loss 1.2578\n",
            "saving checkpoint0\n",
            "iter 0: loss 7.6154, time 9350.05ms\n",
            "iter 1: loss 6.8730, time 6685.46ms\n",
            "iter 2: loss 6.0070, time 7662.65ms\n",
            "iter 3: loss 5.2072, time 5093.17ms\n",
            "iter 4: loss 4.4875, time 3956.44ms\n",
            "iter 5: loss 3.8917, time 3914.69ms\n",
            "iter 6: loss 3.2888, time 5173.60ms\n",
            "iter 7: loss 2.7055, time 3983.01ms\n",
            "iter 8: loss 2.0682, time 3862.40ms\n",
            "iter 10: loss 0.8432, time 4778.31ms\n",
            "iter 11: loss 0.4851, time 3895.20ms\n",
            "iter 12: loss 0.3091, time 3944.90ms\n",
            "iter 13: loss 0.2228, time 5192.24ms\n",
            "iter 14: loss 0.1746, time 3925.91ms\n",
            "iter 15: loss 0.1368, time 3860.68ms\n",
            "iter 16: loss 0.1121, time 4455.50ms\n",
            "iter 17: loss 0.1093, time 4853.14ms\n",
            "iter 18: loss 0.1027, time 3976.14ms\n",
            "iter 19: loss 0.0938, time 4073.41ms\n",
            "iter 20: loss 0.0905, time 5006.36ms\n",
            "iter 21: loss 0.0821, time 3964.04ms\n",
            "iter 22: loss 0.0829, time 3890.95ms\n",
            "iter 23: loss 0.0782, time 4974.62ms\n",
            "iter 24: loss 0.0823, time 3982.23ms\n",
            "iter 25: loss 0.0733, time 3961.88ms\n",
            "iter 26: loss 0.0829, time 4642.13ms\n",
            "iter 27: loss 0.0730, time 4560.26ms\n",
            "iter 28: loss 0.0705, time 3974.12ms\n",
            "iter 29: loss 0.0697, time 4234.08ms\n",
            "iter 30: loss 0.0662, time 4869.81ms\n",
            "iter 31: loss 0.0635, time 3982.15ms\n",
            "iter 32: loss 0.0649, time 4029.10ms\n",
            "iter 33: loss 0.0618, time 5220.55ms\n",
            "iter 34: loss 0.0644, time 3985.08ms\n",
            "iter 35: loss 0.0642, time 3945.14ms\n",
            "iter 36: loss 0.0643, time 4941.99ms\n",
            "iter 37: loss 0.0636, time 4246.64ms\n",
            "iter 38: loss 0.0632, time 3969.24ms\n",
            "iter 39: loss 0.0634, time 4363.41ms\n",
            "iter 40: loss 0.0643, time 4799.75ms\n",
            "iter 41: loss 0.0629, time 3970.46ms\n",
            "iter 42: loss 0.0619, time 4148.18ms\n",
            "iter 43: loss 0.0649, time 5094.46ms\n",
            "iter 44: loss 0.0637, time 4029.67ms\n",
            "iter 45: loss 0.0637, time 4007.93ms\n",
            "iter 46: loss 0.0656, time 5150.47ms\n",
            "iter 47: loss 0.0622, time 4030.95ms\n",
            "iter 48: loss 0.0636, time 4003.36ms\n",
            "iter 49: loss 0.0610, time 4760.43ms\n",
            "iter 50: loss 0.0634, time 4378.97ms\n",
            "iter 51: loss 0.0624, time 4008.41ms\n",
            "iter 52: loss 0.0618, time 4321.30ms\n",
            "iter 53: loss 0.0623, time 4893.24ms\n",
            "iter 54: loss 0.0597, time 4022.15ms\n",
            "iter 55: loss 0.0580, time 4112.99ms\n",
            "iter 56: loss 0.0582, time 5023.14ms\n",
            "iter 57: loss 0.0574, time 4011.56ms\n",
            "iter 58: loss 0.0564, time 4007.11ms\n",
            "iter 59: loss 0.0574, time 5200.44ms\n",
            "iter 60: loss 0.0570, time 4098.31ms\n",
            "iter 61: loss 0.0573, time 4038.28ms\n",
            "iter 62: loss 0.0580, time 4696.35ms\n",
            "iter 63: loss 0.0589, time 4420.08ms\n",
            "iter 64: loss 0.0586, time 4080.22ms\n",
            "iter 65: loss 0.0603, time 4295.02ms\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-ec94b060c429>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-ad9d11ec8b45>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mis_accumulating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfabric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_backward_sync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_accumulating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwandblog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miter_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfabric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-d13a595dd7b8>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.memory_allocated()"
      ],
      "metadata": {
        "id": "bFONsQ--5D3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.reset_max_memory_allocated()"
      ],
      "metadata": {
        "id": "Ez_0g1hJ5JiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gnpi-ce2DGgt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}