{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeEC2x-cPRjZ",
        "outputId": "c22545b1-abf7-40b4-8761-d1b3f5e9c6a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-lightning in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.2)\n",
            "Requirement already satisfied: torch in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.1+cu117)\n",
            "Requirement already satisfied: lightning in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.1.0.dev0)\n",
            "Requirement already satisfied: fabric in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.0.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-lightning) (4.65.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-lightning) (6.0)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-lightning) (2023.5.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-lightning) (0.11.4)\n",
            "Requirement already satisfied: packaging>=17.1 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-lightning) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-lightning) (4.5.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.7.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-lightning) (0.8.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: arrow<3.0,>=1.2.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (1.2.3)\n",
            "Requirement already satisfied: beautifulsoup4<6.0,>=4.8.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (4.12.2)\n",
            "Requirement already satisfied: click<10.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (8.1.3)\n",
            "Requirement already satisfied: croniter<1.4.0,>=1.3.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (1.3.14)\n",
            "Requirement already satisfied: dateutils<2.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (0.6.12)\n",
            "Requirement already satisfied: deepdiff<8.0,>=5.7.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (6.3.0)\n",
            "Requirement already satisfied: fastapi<0.89.0,>=0.69.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (0.88.0)\n",
            "Requirement already satisfied: inquirer<5.0,>=2.10.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (3.1.3)\n",
            "Requirement already satisfied: lightning-cloud>=0.5.34 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (0.5.34)\n",
            "Requirement already satisfied: psutil<7.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (5.9.5)\n",
            "Requirement already satisfied: pydantic<4.0,>=1.7.4 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (1.10.7)\n",
            "Requirement already satisfied: python-multipart<2.0,>=0.0.5 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (0.0.6)\n",
            "Requirement already satisfied: requests<4.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (2.30.0)\n",
            "Requirement already satisfied: rich<15.0,>=12.3.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (13.3.5)\n",
            "Requirement already satisfied: starlette in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (0.22.0)\n",
            "Requirement already satisfied: starsessions<2.0,>=1.2.1 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (1.3.0)\n",
            "Requirement already satisfied: traitlets<7.0,>=5.3.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (5.9.0)\n",
            "Requirement already satisfied: urllib3<3.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (1.26.15)\n",
            "Requirement already satisfied: uvicorn<2.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (0.22.0)\n",
            "Requirement already satisfied: websocket-client<3.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (1.5.1)\n",
            "Requirement already satisfied: websockets<12.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning) (11.0.3)\n",
            "Requirement already satisfied: invoke>=2.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fabric) (2.1.2)\n",
            "Requirement already satisfied: paramiko>=2.4 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fabric) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from arrow<3.0,>=1.2.0->lightning) (2.8.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4<6.0,>=4.8.0->lightning) (2.4.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click<10.0->lightning) (0.4.6)\n",
            "Requirement already satisfied: pytz in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dateutils<2.0->lightning) (2023.3)\n",
            "Requirement already satisfied: ordered-set<4.2.0,>=4.0.2 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from deepdiff<8.0,>=5.7.0->lightning) (4.1.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from starlette->lightning) (3.6.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.4)\n",
            "Requirement already satisfied: blessed>=1.19.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from inquirer<5.0,>=2.10.0->lightning) (1.20.0)\n",
            "Requirement already satisfied: python-editor>=1.0.4 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from inquirer<5.0,>=2.10.0->lightning) (1.0.4)\n",
            "Requirement already satisfied: readchar>=3.0.6 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from inquirer<5.0,>=2.10.0->lightning) (4.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: pyjwt in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning-cloud>=0.5.34->lightning) (2.7.0)\n",
            "Requirement already satisfied: six in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning-cloud>=0.5.34->lightning) (1.16.0)\n",
            "Requirement already satisfied: bcrypt>=3.2 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from paramiko>=2.4->fabric) (4.0.1)\n",
            "Requirement already satisfied: cryptography>=3.3 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from paramiko>=2.4->fabric) (40.0.2)\n",
            "Requirement already satisfied: pynacl>=1.5 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from paramiko>=2.4->fabric) (1.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<4.0->lightning) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<4.0->lightning) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<4.0->lightning) (2023.5.7)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich<15.0,>=12.3.0->lightning) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich<15.0,>=12.3.0->lightning) (2.15.1)\n",
            "Requirement already satisfied: itsdangerous<3.0.0,>=2.0.1 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from starsessions<2.0,>=1.2.1->lightning) (2.1.2)\n",
            "Requirement already satisfied: h11>=0.8 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn<2.0->lightning) (0.14.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5,>=3.4.0->starlette->lightning) (1.3.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from blessed>=1.19.0->inquirer<5.0,>=2.10.0->lightning) (0.2.6)\n",
            "Requirement already satisfied: jinxed>=1.1.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from blessed>=1.19.0->inquirer<5.0,>=2.10.0->lightning) (1.2.0)\n",
            "Requirement already satisfied: cffi>=1.12 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cryptography>=3.3->paramiko>=2.4->fabric) (1.15.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich<15.0,>=12.3.0->lightning) (0.1.2)\n",
            "Requirement already satisfied: setuptools>=41.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from readchar>=3.0.6->inquirer<5.0,>=2.10.0->lightning) (67.8.0)\n",
            "Requirement already satisfied: pycparser in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric) (2.21)\n",
            "Requirement already satisfied: ansicon in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinxed>=1.1.0->blessed>=1.19.0->inquirer<5.0,>=2.10.0->lightning) (1.89.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-lightning torch lightning fabric\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CW0nsmEIvGtg"
      },
      "outputs": [],
      "source": [
        "import lightning.pytorch as pl\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import lightning as L\n",
        "from lightning import Fabric as f\n",
        "import torch.nn as nn\n",
        "from dataclasses import dataclass\n",
        "from torch.nn import functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6JKFl30sAavj"
      },
      "outputs": [],
      "source": [
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GB5XZvepPjhT",
        "outputId": "dd064813-25c4-4de7-a16b-807236c44a66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.1.99)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece\n",
        "import sentencepiece\n",
        "from sentencepiece import SentencePieceProcessor,SentencePieceTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ubPOVrnOA9M_"
      },
      "outputs": [],
      "source": [
        "from typing import Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "-4wZ29CtAv23"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "#Tokenizer[model path]\n",
        "\n",
        "class Tokenizer:\n",
        "\n",
        "  def __init__(self, model_path):\n",
        "    self.processor = SentencePieceProcessor(model_file=str(model_path))\n",
        "    self.bos_id = self.processor.bos_id()\n",
        "    self.eos_id = self.processor.eos_id()\n",
        "    self.pad_id = self.processor.pad_id()\n",
        "  \n",
        "  @property\n",
        "  def vocab_size(self) -> int:\n",
        "    return self.processor.vocab_size()\n",
        "\n",
        "\n",
        "#encode\n",
        "  def encode(\n",
        "        self,\n",
        "        string: str,\n",
        "        bos: bool = False,\n",
        "        eos: bool = False,\n",
        "        max_length: int = -1,\n",
        "        pad: bool = False,\n",
        "        device: Optional[torch.device] = None\n",
        "    ) -> torch.Tensor:\n",
        "    tokens = self.processor.encode(string)\n",
        "    if bos:\n",
        "        tokens = [self.bos_id] + tokens\n",
        "    if eos:\n",
        "        tokens = tokens + [self.eos_id]\n",
        "    if max_length > 0:\n",
        "        tokens = tokens[:max_length]\n",
        "    if pad and len(tokens) < max_length:\n",
        "        tokens += [self.pad_id] * (max_length - len(tokens))\n",
        "\n",
        "    return torch.tensor(tokens, dtype=torch.long, device=device)\n",
        "\n",
        "\n",
        "#decode\n",
        "  def decode(self,tokens:torch.Tensor) -> str:\n",
        "    return self.processor.decode(tokens.tolist())\n",
        "  \n",
        "#train[take input txt bro][have export it on path]\n",
        "  @staticmethod\n",
        "  def train(input: str, destination: str, vocab_size=2000) -> None:\n",
        "    model_prefix = os.path.join(destination, \"tokenizer\")\n",
        "    SentencePieceTrainer.Train(input=input, model_prefix=model_prefix, vocab_size=vocab_size)\n",
        "\n",
        "#@title\n",
        "Tokenizer.train(input=\"C:\\\\Users\\\\SHIVA SINGH\\\\Documents\\\\GitHub\\\\MachineLearningEngine\\\\LLM[Deprecated]\\\\lightningMain\\\\data\\\\input.txt\",destination=\"C:\\\\Users\\\\SHIVA SINGH\\\\Documents\\\\GitHub\\\\MachineLearningEngine\\\\LLM[Deprecated]\\\\lightningMain\\\\tokenizer\")\n",
        "\n",
        "#@title\n",
        "enc = Tokenizer(\"C:\\\\Users\\\\SHIVA SINGH\\\\Documents\\\\GitHub\\\\MachineLearningEngine\\\\LLM[Deprecated]\\\\lightningMain\\\\tokenizer\\\\tokenizer.model\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbQ2GZlABCTP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "bxKvWNneOfWV"
      },
      "outputs": [],
      "source": [
        "scale = 12\n",
        "block_size = 8\n",
        "learning_rate = 6e-4 # max learning rate\n",
        "max_iters = 600 # total number of training iterations\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95 \n",
        "n_layer = 2\n",
        "n_head = 2\n",
        "n_embd = 96\n",
        "betas = (beta1,beta2)\n",
        "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
        "bias = False\n",
        "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
        "block_size = 24\n",
        "vocab_size = 2000\n",
        "out_dir = 'out'\n",
        "eval_interval = 200\n",
        "log_interval = 1\n",
        "eval_iters = 200\n",
        "eval_only = False # if True, script exits right after the first eval\n",
        "always_save_checkpoint = True # if True, always save a checkpoint after each eval\n",
        "init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n",
        "# wandb logging\n",
        "wandb_log = True # disabled by default\n",
        "wandb_project = 'owt'\n",
        "wandb_run_name = 'gpt2' # 'run' + str(time.time())\n",
        "# data\n",
        "dataset = 'openwebtext'\n",
        "gradient_accumulation_steps = 5 * 8 # used to simulate larger batch sizes\n",
        "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
        "block_size = 64\n",
        "# model\n",
        " # do we use bias inside LayerNorm and Linear layers?\n",
        "# adamw optimizer\n",
        "learning_rate = 6e-4 # max learning rate\n",
        "max_iters = 600 # total number of training iterations\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
        "# learning rate decay settings\n",
        "decay_lr = True # whether to decay the learning rate\n",
        "warmup_iters = 20 # how many steps to warm up for\n",
        "lr_decay_iters = 600 # should be ~= max_iters per Chinchilla\n",
        "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
        "# DDP settings\n",
        "backend = 'gloo' # 'nccl', 'gloo', etc.\n",
        "# system\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "dtype = 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "compile = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "MluGtIWPOiKN"
      },
      "outputs": [],
      "source": [
        "config = {k:v for k,v in globals().items() if not k.startswith(\"_\") and isinstance(v,(int,float,bool,str))}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "TT_dkQMFOGkq"
      },
      "outputs": [],
      "source": [
        "class Config(dict):\n",
        "  def __init__(self,config):\n",
        "    self.__dict__.update(config)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "KoAZq7M0Oys_"
      },
      "outputs": [],
      "source": [
        "config = Config(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IppZEadeO1iP",
        "outputId": "a6cfc9f3-f9c1-4eff-a8e2-481634be2776"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config.block_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "jJWoyyVVMbjc"
      },
      "outputs": [],
      "source": [
        "class NextToken(torch.utils.data.Dataset):##custom daatset\n",
        "  def __init__(self,bin_file,config):\n",
        "    super().__init__()\n",
        "    self.data = np.memmap(bin_file,dtype=np.uint16,mode='r')#memmap\n",
        "    self.config = config\n",
        "  \n",
        "  def __len__(self):\n",
        "    return (len(self.data) - self.config.block_size)\n",
        "\n",
        "  def __getitem__(self,i):\n",
        "    data = self.data\n",
        "    start_index = i\n",
        "    end_index = start_index + self.config.block_size\n",
        "    x = torch.from_numpy((self.data[start_index:end_index]).astype(np.int64))\n",
        "    y = torch.from_numpy((self.data[start_index + 1:end_index + 1]).astype(np.int64))\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "a8nxqHWihUfL"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.distributed import DistributedSampler\n",
        "class Sampler(DistributedSampler):\n",
        "  def __init__(self,dataset,world_size,rank,shuffle):\n",
        "    super().__init__(dataset=dataset,num_replicas=world_size,rank=rank,shuffle=shuffle)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "o3nydHcqmj0e"
      },
      "outputs": [],
      "source": [
        "class DataModule(pl.LightningDataModule):\n",
        "  def __init__(self,dir_path,config,trainer=None):\n",
        "    self.dir_path = dir_path\n",
        "    self.prepare_data_per_node = True\n",
        "    self.config = config\n",
        "    self.trainer = trainer\n",
        "    self.current_epoch = 0\n",
        "  \n",
        "  def prepare_data(self):\n",
        "    input_file_path = os.path.join(self.dir_path,\"input.txt\")\n",
        "\n",
        "    with open(input_file_path,'r') as f:\n",
        "      text = f.read()\n",
        "    n = len(text)\n",
        "\n",
        "    train_data = text[:int(n*0.9)]\n",
        "    val_data = text[int(n*0.9):]\n",
        "    #encode\n",
        "    train_ids = enc.encode(train_data)\n",
        "    val_ids = enc.encode(val_data)\n",
        "    #print\n",
        "    if self.trainer.is_global_zero:\n",
        "      print(f\"train_ids has {len(train_ids)} tokens\")\n",
        "      print(f\"val_ids has {len(val_ids)} tokens\")\n",
        "    #convert to numpy\n",
        "    train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "    val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "    #export_to_file\n",
        "    train_ids.tofile(os.path.join(self.dir_path,'train.bin'))\n",
        "    val_ids.tofile(os.path.join(self.dir_path,'val.bin'))\n",
        "\n",
        "  def setup(self, stage):\n",
        "    if stage == \"fit\":\n",
        "      self.train_data = NextToken(bin_file=os.path.join(self.dir_path, \"train.bin\"), config=self.config)#train_dataset = torch.utild.dataset(nextTokendataset)\n",
        "      if self.trainer.is_global_zero:\n",
        "        self.val_data = NextToken(bin_file=os.path.join(self.dir_path, \"val.bin\"), config=self.config)\n",
        "    \n",
        "    if stage == \"predict\":\n",
        "      self.predict_data = NextToken(bin_file=os.path.join(self.dir_path, \"predict.bin\"), config=self.config)\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    if isinstance(self.trainer,pl.Trainer):\n",
        "      return DataLoader(self.train_data, batch_size=64, pin_memory=True, num_workers=0,shuffle=True)\n",
        "    else:\n",
        "      self.sampler = Sampler(dataset=self.train_data,world_size=self.trainer.world_size, rank=self.trainer.local_rank, shuffle=True)\n",
        "      self.sampler.set_epoch(self.current_epoch)\n",
        "      return DataLoader(self.train_data, batch_size=64, pin_memory=True, num_workers=0, sampler=self.sampler)  \n",
        "  \n",
        "  def val_dataloader(self):\n",
        "    if self.trainer.is_global_zero:\n",
        "      return DataLoader(self.val_data, batch_size=64, pin_memory=False, num_workers=0,shuffle=True)\n",
        "    else:\n",
        "      print(\"only master node have val_data\")\n",
        "      return None\n",
        "  \n",
        "  def on_epoch_start(self):\n",
        "    self.current_eoch = self.trainer.current_epoch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0mQYJQ5OZ08",
        "outputId": "6e240999-c0ee-40bb-982e-8ba33321b35c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "trainer = pl.Trainer(accelerator=\"cuda\",max_epochs=1,use_distributed_sampler=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3oA2ossXHfc",
        "outputId": "79e32157-40fd-4ca1-d9fe-c2db5aec5db1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.current_epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "yga-nkNmvBDY"
      },
      "outputs": [],
      "source": [
        "dm  = DataModule(\"C:\\\\Users\\\\SHIVA SINGH\\\\Documents\\\\GitHub\\\\MachineLearningEngine\\\\LLM[Deprecated]\\\\lightningMain\\\\data\" ,config=config,trainer=trainer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KGiM0emQVh7",
        "outputId": "44f01e17-4a9e-4c27-baef-d5db198b6fb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_ids has 18952 tokens\n",
            "val_ids has 2137 tokens\n"
          ]
        }
      ],
      "source": [
        "dm.prepare_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "DikPLffnJNsg"
      },
      "outputs": [],
      "source": [
        "dm.setup(stage=\"fit\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "VGzFgGfseQ8T"
      },
      "outputs": [],
      "source": [
        "dm.current_epoch = 12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "48DkHRzhKMiD"
      },
      "outputs": [],
      "source": [
        "dt = dm.train_dataloader()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bxxvAX9bBEv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74TPJPi_YYIm",
        "outputId": "1d4796f0-a90a-4f60-a8eb-3687b8f2aa75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[  11,  351,   24,  ...,  185,  146,   23],\n",
            "        [   7,  298,   12,  ...,  173,    7,   51],\n",
            "        [  16,  979,  565,  ...,   19, 1069,   29],\n",
            "        ...,\n",
            "        [   3,  149,  710,  ...,   30,   31,    3],\n",
            "        [   7,   62,  555,  ...,   23,    6,  259],\n",
            "        [  54,   17, 1381,  ...,  127,    5,  449]]) tensor([[ 351,   24,   16,  ...,  146,   23,   79],\n",
            "        [ 298,   12,  956,  ...,    7,   51,  853],\n",
            "        [ 979,  565,  638,  ..., 1069,   29,   20],\n",
            "        ...,\n",
            "        [ 149,  710,  113,  ...,   31,    3,   92],\n",
            "        [  62,  555,    3,  ...,    6,  259,   11],\n",
            "        [  17, 1381,  301,  ...,    5,  449,   11]])\n"
          ]
        }
      ],
      "source": [
        "for a,b in dt:\n",
        "  print(a,b)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "th7fa8RaYVeL",
        "outputId": "3e169d1d-426e-4866-91f3-dbfb41fc8687"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 135,    4,    7,  ...,   11,    6, 1780],\n",
            "        [  15,   41,  339,  ...,   10,  196,   31],\n",
            "        [   4,   28, 1859,  ...,  251,  116,   24],\n",
            "        ...,\n",
            "        [  33,   94,  314,  ...,    9,  269,   13],\n",
            "        [ 196,   13,   10,  ...,    8, 1640,   10],\n",
            "        [  13,  167,  422,  ...,  115,   12,  111]]) tensor([[   4,    7,  473,  ...,    6, 1780,  371],\n",
            "        [  41,  339,  166,  ...,  196,   31,   15],\n",
            "        [  28, 1859, 1363,  ...,  116,   24,  394],\n",
            "        ...,\n",
            "        [  94,  314,  146,  ...,  269,   13,   30],\n",
            "        [  13,   10,   68,  ..., 1640,   10,   93],\n",
            "        [ 167,  422,  974,  ...,   12,  111,  179]])\n"
          ]
        }
      ],
      "source": [
        "dm.current_epoch = 9\n",
        "for a,b in dt:\n",
        "  print(a,b)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "KcLBu465yDh5"
      },
      "outputs": [],
      "source": [
        "#########MODEL#####################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "cx3G6Zzs0xHX"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    assert config.n_embd & config.n_head == 0\n",
        "    self.c_attn = nn.Linear(config.n_embd,3*config.n_embd,bias=config.bias)\n",
        "    self.c_proj = nn.Linear(config.n_embd,config.n_embd,bias=config.bias)\n",
        "    self.attn_dropout = nn.Dropout(config.dropout)\n",
        "    self.resid_dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    self.n_head = config.n_head\n",
        "    self.n_embd = config.n_embd\n",
        "    self.dropout = config.dropout\n",
        "\n",
        "  def forward(self,x):#B,T,C\n",
        "    B,T,C = x.size()\n",
        "    q,k,v = self.c_attn(x).split(self.n_embd,dim=2)\n",
        "    q = q.view(B,T,self.n_head,C//self.n_head).transpose(1,2)\n",
        "    k = k.view(B,T,self.n_head,C//self.n_head).transpose(1,2)\n",
        "    v = v.view(B,T,self.n_head,C//self.n_head).transpose(1,2)#B T nh headDim->b nh t hs\n",
        "\n",
        "    y = torch.nn.functional.scaled_dot_product_attention(q,k,v,attn_mask=None,dropout_p=self.dropout if self.training else 0,is_causal=True) #B NH T HS\n",
        "    y = y.transpose(1,2).contiguous().view(B,T,C)\n",
        "\n",
        "    out = self.c_proj(y)\n",
        "    out = self.resid_dropout(y)\n",
        "    return out #b  t c\n",
        "\n",
        "#@title\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    self.c_fc   = nn.Linear(config.n_embd, 4 * config.n_embd,bias=config.bias)\n",
        "    self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd,bias=config.bias)\n",
        "    self.dropout= nn.Dropout(config.dropout)\n",
        "    self.act = nn.GELU()\n",
        "  def forward(self,x):\n",
        "    x = self.dropout(self.c_proj(self.act(self.c_fc(x))))\n",
        "    return x\n",
        "\n",
        "#@title\n",
        "class Block(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    self.attn = CausalSelfAttention(config)\n",
        "    self.ln_1 = LayerNorm(config.n_embd,config.bias)\n",
        "    self.ln_2 = LayerNorm(config.n_embd,config.bias)\n",
        "    self.mlp = MLP(config) # layernorm 2 a attention a mlp\n",
        "  \n",
        "  def forward(self,x):\n",
        "    x = x + self.attn(self.ln_1(x))\n",
        "    x = x + self.mlp(self.ln_2(x))\n",
        "    return x\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self,ndim,bias):\n",
        "    super().__init__()\n",
        "    self.weight = nn.Parameter(torch.ones(ndim))\n",
        "    self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "  \n",
        "  def forward(self,x):\n",
        "    return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "-X3XSduiyFq3"
      },
      "outputs": [],
      "source": [
        "class plModel(pl.LightningModule):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    self.losslist = []\n",
        "    self.config = config\n",
        "    print(f\"this model is on {self.device}\")\n",
        "    self.transformer = nn.ModuleDict(dict(\n",
        "        wte = nn.Embedding(config.vocab_size,config.n_embd),\n",
        "        wpe = nn.Embedding(config.block_size,config.n_embd),\n",
        "        h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "        drop = nn.Dropout(config.dropout),\n",
        "        ln_f = LayerNorm(config.n_embd,config.bias)\n",
        "    ))\n",
        "    self.lm_head = nn.Linear(config.n_embd,config.vocab_size,bias=False)\n",
        "\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "    for a,b in self.named_parameters():\n",
        "      if a.endswith(\"c_proj.weight\"):\n",
        "        torch.nn.init.normal_(b,mean=0.0,std=0.02/math.sqrt(2 * config.n_layer))\n",
        "  \n",
        "    print(sum(p.numel() for p in self.parameters()))\n",
        "\n",
        "  def _init_weights(self,module):\n",
        "    if isinstance(module,nn.Linear):\n",
        "      torch.nn.init.normal_(module.weight,mean=0.0,std=0.02)\n",
        "      if module.bias is not None:\n",
        "        torch.nn.init.zeros_(module.bias)\n",
        "    if isinstance(module,nn.Embedding):\n",
        "      torch.nn.init.normal_(module.weight,mean=0.0,std=0.02)\n",
        "    \n",
        "    \n",
        "  def forward(self,x,y=None):#X=[B,CONTEXT_SIZE] = B,T\n",
        "    b,t = x.size()\n",
        "    pos = torch.arange(0,t,dtype=torch.int32, device=self.device).unsqueeze(0)\n",
        "    t_emb = self.transformer.wte(x)\n",
        "    #B,CONTEXT_SIZE,VOCAB_SIZE = B,T,C\n",
        "    p_emb = self.transformer.wpe(pos)\n",
        "    x = self.transformer.drop(t_emb+p_emb)\n",
        "    for block in self.transformer.h:\n",
        "      x = block(x) # b t c\n",
        "    out = self.transformer.ln_f(x)###BTC WE HAVE TO PASS THRROUGH LM_HEAD FOR B T VOCAB_SIZE\n",
        "    logits = self.lm_head(out)\n",
        "    return logits\n",
        "  \n",
        "  def training_step(self,batch,batch_idx):\n",
        "    x,y = batch\n",
        "    x = x.to(self.device)\n",
        "    y = y.to(self.device)\n",
        "    logits = self(x)#b t vocab_dim\n",
        "    B,T,vdim = logits.shape\n",
        "\n",
        "    logits = logits.view(B*T,vdim)\n",
        "    target = y.view(B*T)\n",
        "    loss = F.cross_entropy(logits,target)\n",
        "    self.losslist.append(loss)\n",
        "    return loss\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "    param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "    decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "    nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "    optim_groups = [\n",
        "        {'params': decay_params, 'weight_decay': self.config.weight_decay},\n",
        "        {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "    ]\n",
        "    num_decay_params = sum(p.numel() for p in decay_params)\n",
        "    num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "    print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "    print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "    optimizer = torch.optim.AdamW(optim_groups, lr=self.config.learning_rate, betas=(self.config.beta1,self.config.beta2))\n",
        "\n",
        "    return optimizer\n",
        " \n",
        "\n",
        "  @torch.no_grad()\n",
        "  def generate(self,idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "      logits, _ = self(idx_cond)\n",
        "      logits = logits[:, -1, :] / temperature\n",
        "      if top_k is not None:\n",
        "          v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "          logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "    \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "1VuixlQM6_gV"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "scale = 6\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "  block_size:int=1024\n",
        "  vocab_size:int=50304\n",
        "  n_layer:int=12//scale\n",
        "  n_embd:int=768//scale\n",
        "  n_head:int=12//scale\n",
        "  dropout:float=0.0\n",
        "  bias:bool=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWW7-r1n7m-T",
        "outputId": "fd0be3fd-538e-49df-8f06-e6aa80848366"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "this model is on cpu\n",
            "611808\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = plModel(config)\n",
        "model.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "bK-6SMaO7zkk"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name        | Type       | Params\n",
            "-------------------------------------------\n",
            "0 | transformer | ModuleDict | 419 K \n",
            "1 | lm_head     | Linear     | 192 K \n",
            "-------------------------------------------\n",
            "611 K     Trainable params\n",
            "0         Non-trainable params\n",
            "611 K     Total params\n",
            "2.447     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num decayed parameter tensors: 11, with 611,328 parameters\n",
            "num non-decayed parameter tensors: 5, with 480 parameters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\SHIVA SINGH\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:435: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "effc35504c6740cf911f5df5c9d98c3a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
          ]
        }
      ],
      "source": [
        "trainer.fit(model,dt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "AjHMLm6mmRhw"
      },
      "outputs": [],
      "source": [
        "##############Trainer######################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "qyFC2nRImkXs"
      },
      "outputs": [],
      "source": [
        "#now approach is forget about data and model, focus on trainer only using fabric customed thats it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FKbLkw_Hb2C",
        "outputId": "f52d58ed-1958-41a0-fa2c-4cbc262d4c1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.15.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (3.1.31)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (2.30.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (1.23.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: pathtools in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: setuptools in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (67.8.0)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (4.23.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Click!=8.0.0,>=7.0->wandb) (0.4.6)\n",
            "Requirement already satisfied: six>=1.4.0 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2023.5.7)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\shiva singh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIKsAa2Xxavt",
        "outputId": "08474351-0eae-402c-9d45-20f179caf7af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "nsCQo14Mx-vs"
      },
      "outputs": [],
      "source": [
        "import lightning as L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "sAOz57uGxQRV"
      },
      "outputs": [],
      "source": [
        "from lightning.fabric import Fabric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "mL9bzDwmy6tA"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "39FzmCUMmr4m"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class Trainer:\n",
        "  def __init__(self,config,model,datamodule):\n",
        "    \n",
        "    self.config = config\n",
        "    self.fabric = Fabric(precision=\"16-mixed\",accelerator=\"cuda\",strategy=\"auto\")\n",
        "    self.model = model\n",
        "    self.optimizer = model.configure_optimizers()\n",
        "    self.device = self.fabric.device\n",
        "    self.dm = datamodule(dir_path=\"C:\\\\Users\\\\SHIVA SINGH\\\\Documents\\\\GitHub\\\\MachineLearningEngine\\\\LLM[Deprecated]\\\\lightningMain\\\\data\",config=self.config,trainer=self.fabric)\n",
        "    #self.dm.prepare_data()\n",
        "    self.dm.setup(stage=\"fit\")\n",
        "    self.dm.setup(stage=\"val\")\n",
        "\n",
        "  def train(self):\n",
        "    \n",
        "    master_process = self.fabric.global_rank == 0\n",
        "\n",
        "    if master_process:\n",
        "      os.makedirs(self.config.out_dir, exist_ok=True)\n",
        "    \n",
        "    self.fabric.seed_everything(1227+self.fabric.global_rank)\n",
        "\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "    iter_num = 0\n",
        "    best_val_loss = 1e9\n",
        "    t0 = time.time()\n",
        "\n",
        "    if self.config.wandb_log and master_process:\n",
        "      wandb.init(project=self.config.wandb_project, name=self.config.wandb_run_name)\n",
        "\n",
        "    self.model, self.optimizer = self.fabric.setup(self.model, self.optimizer)\n",
        "\n",
        "    for iter_num in range(self.config.max_iters):\n",
        "      \n",
        "      lr = self.get_lr(iter_num,self.config) if self.config.decay_lr else self.config.learning_rate\n",
        "      for param_group in self.optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "  \n",
        "      \n",
        "      if iter_num % self.config.eval_interval == 0 and master_process:\n",
        "        losses = self.estimate_loss(self.config)\n",
        "        print(f\"step {iter_num}: eval_loss {losses['val']:.4f}\")\n",
        "\n",
        "      \n",
        "        if losses[\"val\"] < best_val_loss or self.always_save_checkpoints:\n",
        "          best_val_loss = losses[\"val\"]\n",
        "          self.checkpoint(model = self.model.state_dict(),iter=iter_num)\n",
        "\n",
        "     \n",
        "      \n",
        "\n",
        "      self.optimizer.zero_grad(set_to_none=True)\n",
        "      \n",
        "      self.model.current_epoch = iter_num\n",
        "      for batch_idx, batch in enumerate(self.dm.train_dataloader()):\n",
        "        is_accumulating = batch_idx < self.config.gradient_accumulation_steps \n",
        "        with self.fabric.no_backward_sync(self.model,enabled=(is_accumulating)):\n",
        "          loss = self.model.training_step(batch, batch_idx)\n",
        "          self.wandblog(iter=iter_num,loss=loss)\n",
        "          self.fabric.backward(loss)\n",
        "        \n",
        "        if not is_accumulating:\n",
        "          self.optimizer.step()\n",
        "          self.optimizer.zero_grad()\n",
        "      \n",
        "\n",
        "      t1 = time.time()\n",
        "      dt = t1 - t0\n",
        "      t0 = t1\n",
        "\n",
        "      if master_process:\n",
        "        lossf = loss.item()\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms\")\n",
        "      \n",
        "  @torch.no_grad()\n",
        "  def estimate_loss(self,config):\n",
        "    out = {}\n",
        "    self.model.eval()\n",
        "    for split in [\"val\"]:\n",
        "      losses = torch.zeros(config.eval_iters)\n",
        "      for k,batch in enumerate(self.dm.val_dataloader()):\n",
        "        loss = model.training_step(batch, k)\n",
        "        losses[k] = loss.item()\n",
        "        if k>config.eval_iters:\n",
        "          break\n",
        "      out[split] = losses.mean()\n",
        "    self.model.train()\n",
        "    return out\n",
        "  \n",
        "  def get_lr(self,iter,config):\n",
        "    if iter < config.warmup_iters:\n",
        "        return config.learning_rate * iter / config.warmup_iters\n",
        "    if iter > config.lr_decay_iters:\n",
        "        return config.min_lr\n",
        "    decay_ratio = (iter - config.warmup_iters) / (config.lr_decay_iters - config.warmup_iters)\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return config.min_lr + coeff * (config.learning_rate - config.min_lr)\n",
        "  \n",
        "  def checkpoint(self,model,iter):\n",
        "    checkpoint = {\n",
        "        \"model\":model,\n",
        "        \"iter_num\":iter,\n",
        "    }\n",
        "    print(f\"saving checkpoint{iter}\")\n",
        "    torch.save(checkpoint, os.path.join(out_dir, \"ckpt.pt\"))\n",
        "  \n",
        "  def wandblog(self,iter,loss):\n",
        "    wandb.log({\n",
        "        \"iter\": iter,\n",
        "        \"train_loss\": loss,\n",
        "    })\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xy-L3Hem3ipa",
        "outputId": "b6e3eddb-4ab6-422a-bb13-e02fa733cbd9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using 16-bit Automatic Mixed Precision (AMP)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num decayed parameter tensors: 11, with 611,328 parameters\n",
            "num non-decayed parameter tensors: 5, with 480 parameters\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1 t = Trainer(config, model, DataModule)                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">13</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 10 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.device = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.fabric.device                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 11 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dm = datamodule(dir_path=<span style=\"color: #808000; text-decoration-color: #808000\">\"C:\\\\Users\\\\SHIVA SINGH\\\\OneDrive\\\\Documents\\\\LLM\\\\dat</span>   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 12 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">#self.dm.prepare_data()</span>                                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span> 13 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dm.setup(stage=<span style=\"color: #808000; text-decoration-color: #808000\">\"fit\"</span>)                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 14 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dm.setup(stage=<span style=\"color: #808000; text-decoration-color: #808000\">\"val\"</span>)                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 15 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  </span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 16 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>):                                                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">setup</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">36</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">33 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> stage == <span style=\"color: #808000; text-decoration-color: #808000\">\"fit\"</span>:                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">34 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">     </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.train_data = NextToken(bin_file=os.path.join(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dir_path, <span style=\"color: #808000; text-decoration-color: #808000\">\"train.bin\"</span>), con    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">35 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">     </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.trainer.is_global_zero:                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>36 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.val_data = NextToken(bin_file=os.path.join(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dir_path, <span style=\"color: #808000; text-decoration-color: #808000\">\"val.bin\"</span>), confi    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">37 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">38 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> stage == <span style=\"color: #808000; text-decoration-color: #808000\">\"predict\"</span>:                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">39 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">     </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.predict_data = NextToken(bin_file=os.path.join(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dir_path, <span style=\"color: #808000; text-decoration-color: #808000\">\"predict.bin\"</span>),    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">4</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">class</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; text-decoration: underline\">NextToken</span>(torch.utils.data.Dataset):<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">##custom daatset</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>,bin_file,config):                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">super</span>().<span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>()                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span> 4 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.data = np.memmap(bin_file,dtype=np.uint16,mode=<span style=\"color: #808000; text-decoration-color: #808000\">'r'</span>)<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">#memmap</span>                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.config = config                                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__len__</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>):                                                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\SHIVA </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">SINGH\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\memmap.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">228</span> in      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__new__</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">225 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">hasattr</span>(filename, <span style=\"color: #808000; text-decoration-color: #808000\">'read'</span>):                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">226 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>f_ctx = nullcontext(filename)                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">227 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>228 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>f_ctx = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">open</span>(os_fspath(filename), (<span style=\"color: #808000; text-decoration-color: #808000\">'r'</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> mode == <span style=\"color: #808000; text-decoration-color: #808000\">'c'</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> mode)+<span style=\"color: #808000; text-decoration-color: #808000\">'b'</span>)          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">229 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">230 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> f_ctx <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> fid:                                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">231 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>fid.seek(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>)                                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">FileNotFoundError: </span><span style=\"font-weight: bold\">[</span>Errno <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]</span> No such file or directory: <span style=\"color: #008000; text-decoration-color: #008000\">'C:\\\\Users\\\\SHIVA </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">SINGH\\\\OneDrive\\\\Documents\\\\LLM\\\\data\\\\val.bin'</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1 t = Trainer(config, model, DataModule)                                                       \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m in \u001b[92m__init__\u001b[0m:\u001b[94m13\u001b[0m                                                                                   \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 10 \u001b[0m\u001b[2m   \u001b[0m\u001b[96mself\u001b[0m.device = \u001b[96mself\u001b[0m.fabric.device                                                       \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 11 \u001b[0m\u001b[2m   \u001b[0m\u001b[96mself\u001b[0m.dm = datamodule(dir_path=\u001b[33m\"\u001b[0m\u001b[33mC:\u001b[0m\u001b[33m\\\\\u001b[0m\u001b[33mUsers\u001b[0m\u001b[33m\\\\\u001b[0m\u001b[33mSHIVA SINGH\u001b[0m\u001b[33m\\\\\u001b[0m\u001b[33mOneDrive\u001b[0m\u001b[33m\\\\\u001b[0m\u001b[33mDocuments\u001b[0m\u001b[33m\\\\\u001b[0m\u001b[33mLLM\u001b[0m\u001b[33m\\\\\u001b[0m\u001b[33mdat\u001b[0m   \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 12 \u001b[0m\u001b[2m   \u001b[0m\u001b[2m#self.dm.prepare_data()\u001b[0m                                                                \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 13 \u001b[2m   \u001b[0m\u001b[96mself\u001b[0m.dm.setup(stage=\u001b[33m\"\u001b[0m\u001b[33mfit\u001b[0m\u001b[33m\"\u001b[0m)                                                             \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 14 \u001b[0m\u001b[2m   \u001b[0m\u001b[96mself\u001b[0m.dm.setup(stage=\u001b[33m\"\u001b[0m\u001b[33mval\u001b[0m\u001b[33m\"\u001b[0m)                                                             \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 15 \u001b[0m\u001b[2m  \u001b[0m                                                                                         \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 16 \u001b[0m\u001b[2m  \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mtrain\u001b[0m(\u001b[96mself\u001b[0m):                                                                         \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m in \u001b[92msetup\u001b[0m:\u001b[94m36\u001b[0m                                                                                      \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m33 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mif\u001b[0m stage == \u001b[33m\"\u001b[0m\u001b[33mfit\u001b[0m\u001b[33m\"\u001b[0m:                                                                      \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m34 \u001b[0m\u001b[2m     \u001b[0m\u001b[96mself\u001b[0m.train_data = NextToken(bin_file=os.path.join(\u001b[96mself\u001b[0m.dir_path, \u001b[33m\"\u001b[0m\u001b[33mtrain.bin\u001b[0m\u001b[33m\"\u001b[0m), con    \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m35 \u001b[0m\u001b[2m     \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.trainer.is_global_zero:                                                       \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[31m \u001b[0m36 \u001b[2m      \u001b[0m\u001b[96mself\u001b[0m.val_data = NextToken(bin_file=os.path.join(\u001b[96mself\u001b[0m.dir_path, \u001b[33m\"\u001b[0m\u001b[33mval.bin\u001b[0m\u001b[33m\"\u001b[0m), confi    \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m37 \u001b[0m\u001b[2m   \u001b[0m                                                                                        \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m38 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mif\u001b[0m stage == \u001b[33m\"\u001b[0m\u001b[33mpredict\u001b[0m\u001b[33m\"\u001b[0m:                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m39 \u001b[0m\u001b[2m     \u001b[0m\u001b[96mself\u001b[0m.predict_data = NextToken(bin_file=os.path.join(\u001b[96mself\u001b[0m.dir_path, \u001b[33m\"\u001b[0m\u001b[33mpredict.bin\u001b[0m\u001b[33m\"\u001b[0m),    \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m in \u001b[92m__init__\u001b[0m:\u001b[94m4\u001b[0m                                                                                    \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 1 \u001b[0m\u001b[94mclass\u001b[0m \u001b[4;92mNextToken\u001b[0m(torch.utils.data.Dataset):\u001b[2m##custom daatset\u001b[0m                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 2 \u001b[0m\u001b[2m  \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m__init__\u001b[0m(\u001b[96mself\u001b[0m,bin_file,config):                                                       \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 3 \u001b[0m\u001b[2m   \u001b[0m\u001b[96msuper\u001b[0m().\u001b[92m__init__\u001b[0m()                                                                      \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 4 \u001b[2m   \u001b[0m\u001b[96mself\u001b[0m.data = np.memmap(bin_file,dtype=np.uint16,mode=\u001b[33m'\u001b[0m\u001b[33mr\u001b[0m\u001b[33m'\u001b[0m)\u001b[2m#memmap\u001b[0m                         \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 5 \u001b[0m\u001b[2m   \u001b[0m\u001b[96mself\u001b[0m.config = config                                                                    \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 6 \u001b[0m\u001b[2m  \u001b[0m                                                                                          \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 7 \u001b[0m\u001b[2m  \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m__len__\u001b[0m(\u001b[96mself\u001b[0m):                                                                        \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\SHIVA \u001b[0m                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[33mSINGH\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\memmap.py\u001b[0m:\u001b[94m228\u001b[0m in      \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[92m__new__\u001b[0m                                                                                          \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m225 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[96mhasattr\u001b[0m(filename, \u001b[33m'\u001b[0m\u001b[33mread\u001b[0m\u001b[33m'\u001b[0m):                                                      \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m226 \u001b[0m\u001b[2m         \u001b[0mf_ctx = nullcontext(filename)                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m227 \u001b[0m\u001b[2m      \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[31m \u001b[0m228 \u001b[2m         \u001b[0mf_ctx = \u001b[96mopen\u001b[0m(os_fspath(filename), (\u001b[33m'\u001b[0m\u001b[33mr\u001b[0m\u001b[33m'\u001b[0m \u001b[94mif\u001b[0m mode == \u001b[33m'\u001b[0m\u001b[33mc\u001b[0m\u001b[33m'\u001b[0m \u001b[94melse\u001b[0m mode)+\u001b[33m'\u001b[0m\u001b[33mb\u001b[0m\u001b[33m'\u001b[0m)          \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m229 \u001b[0m\u001b[2m      \u001b[0m                                                                                   \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m230 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mwith\u001b[0m f_ctx \u001b[94mas\u001b[0m fid:                                                                 \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m231 \u001b[0m\u001b[2m         \u001b[0mfid.seek(\u001b[94m0\u001b[0m, \u001b[94m2\u001b[0m)                                                                 \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m\n",
              "\u001b[1;91mFileNotFoundError: \u001b[0m\u001b[1m[\u001b[0mErrno \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m No such file or directory: \u001b[32m'C:\\\\Users\\\\SHIVA \u001b[0m\n",
              "\u001b[32mSINGH\\\\OneDrive\\\\Documents\\\\LLM\\\\data\\\\val.bin'\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "t = Trainer(config, model, DataModule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_e3-Nr-C3k-o",
        "outputId": "ad1a27ed-b52f-4363-cde4-c91919c7aaa9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1 t.train()                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'t'</span> is not defined\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1 t.train()                                                                                    \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m\n",
              "\u001b[1;91mNameError: \u001b[0mname \u001b[32m't'\u001b[0m is not defined\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "t.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "bFONsQ--5D3p"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.memory_allocated()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Ez_0g1hJ5JiH"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\SHIVA SINGH\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\cuda\\memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.reset_max_memory_allocated()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gnpi-ce2DGgt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
